{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommenders 4 : Pytorch and Recommenders (~1h)\n",
    "\n",
    "In this practical session, we dive a little more into [pytorch](https://pytorch.org/docs/stable/index.html) and propose to re-implement two classical matrix-factorization models with this neural network toolkit.\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "- (a) See a bit of simple pytorch (~5min)\n",
    "- (b) Discover the \"autograd\" part of pytorch to build a simple baseline (~20min)\n",
    "- (c) Discover the \"nn\" part of pytorch to build a simple matrix factorization algorithm (~20min)\n",
    "- (d) Learn to use a high level framework for pytorch (kind of \"KERAS\" like) to build more complicated algorithms (~15min)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install torch torchvision pytorch-lightning --upgrade\n",
    "#! pip install matplotlib --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a) WHAT IS PYTORCH?\n",
    "\n",
    "It’s a Python-based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "- A replacement for NumPy to use the power of GPUs\n",
    "- a deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "### Tensors : the main unit\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.\n",
    "\n",
    "\n",
    "## Some useful functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize an empty 4x2 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [2.1037e+23, 1.3109e-08]])\n"
     ]
    }
   ],
   "source": [
    "x_empty = torch.empty(4, 2)\n",
    "print(x_empty)  #Tensor is not initialized => contains gibberish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a 3x2 tensor filled with zeros of type long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x0 = torch.zeros(3, 2, dtype=torch.long)\n",
    "print(x0) #Tensor has only zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a tensor of size 2 with (0 => 5.5) and (1 => 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x_data = torch.tensor([5.5, 3])\n",
    "print(x_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x_data = torch.tensor(np.array([5.5, 3])) #also works with numpy arrays\n",
    "print(x_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create random 5x3 and 3x5 tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2876, 0.7139, 0.9825],\n",
      "        [0.4960, 0.1611, 0.0950],\n",
      "        [0.9665, 0.0106, 0.6137],\n",
      "        [0.7154, 0.1236, 0.2365],\n",
      "        [0.5116, 0.6027, 0.5139]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "y = torch.rand(3,5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing works just like numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7139, 0.1611, 0.0106, 0.1236, 0.6027])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,1] #The 2nd column (indexing starts at 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4960, 0.1611, 0.0950],\n",
       "        [0.7154, 0.1236, 0.2365]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[1,3],:] # the 2nd and 4th row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor([1])\n",
    "print(scalar.item()) # Gets the value when tensor is a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### know the size of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size() ## equivalent to x.shape in numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2876, 1.7139, 1.9825],\n",
       "        [1.4960, 1.1611, 1.0950],\n",
       "        [1.9665, 1.0106, 1.6137],\n",
       "        [1.7154, 1.1236, 1.2365],\n",
       "        [1.5116, 1.6027, 1.5139]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+1        # same as x.add(1)\n",
    "x.add_(1)  # inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7803, 4.2155, 1.6433, 2.5044, 2.6663],\n",
       "        [1.4030, 3.1427, 1.3805, 1.7470, 2.0199],\n",
       "        [1.5840, 3.7689, 1.7512, 2.0789, 2.3554],\n",
       "        [1.4892, 3.3875, 1.5319, 1.8668, 2.1597],\n",
       "        [1.7259, 3.9123, 1.6061, 2.2480, 2.5117]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(x,y) # same as x @ y or np.dot(x.numpy(),y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to understand:\n",
    "\n",
    "Pytorch can be a drop-in replacement for numpy. It behaves mostly the same and the API is close.\n",
    "\n",
    "\n",
    "### There are many more creation/operation ops:\n",
    "\n",
    "=> You can have a look at the [torch.Tensor documentation](https://pytorch.org/docs/stable/tensors.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's interesting beyond the \"numpy replacement\": autodiff !\n",
    "\n",
    "Pytorch has Automatic differentiation: You only have to compute a loss function to obtain gradients automatically. How it works is detailed [here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-tensors-and-autograd)\n",
    "\n",
    "### Let's do 1d-linear regression with the vanilla autodiff !\n",
    "\n",
    "#### (First) we need fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xb6ab9c8>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake_x = np.arange(0,100,1)\n",
    "fake_y = np.arange(0,10,0.1) + np.random.rand(100)\n",
    "plt.plot(fake_x,fake_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Here, we won't split the data in train/val/test, this is just an example\n",
    "\n",
    "So, the linear model we want to learn is the following:\n",
    "$$f(x) =  wx+b $$\n",
    "The parameters to optimize are w and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create to tensor variables (which are our parameters)\n",
    "w = torch.tensor([1.],requires_grad=True) # We need to set requires_grad to True so the gradient can flow.\n",
    "b = torch.tensor([0.5],requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the f function:\n",
    "def f(x):\n",
    "    return (w*x)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define an error function (here the MAE)\n",
    "def error(pred,real):\n",
    "    return (pred-real).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "loss: 32.41171314179897\n",
      "w: 0.5049999952316284\n",
      "b: 0.49020129442214966\n",
      "----\n",
      "loss: 8.318853213191032\n",
      "w: 0.10500003397464752\n",
      "b: 0.4814024567604065\n",
      "----\n",
      "loss: 0.3598210698366165\n",
      "w: 0.1056000292301178\n",
      "b: 0.4814024567604065\n",
      "----\n",
      "loss: 0.36155484259128573\n",
      "w: 0.1064000278711319\n",
      "b: 0.4814024567604065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xc0df248>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finally, we cycle through the data, optimizing the parameters with respect to the gradient of the error:\n",
    "plt.plot(fake_y)\n",
    "\n",
    "for epoch in range(4): # We cycle 4 times\n",
    "    mean_loss = 0\n",
    "    \n",
    "    for x,y in zip(fake_x,fake_y): \n",
    "        \n",
    "        pred = f(x) #predict\n",
    "        loss = error(pred,y) #compute loss\n",
    "        loss.backward() # This does backpropagation and sets .grad attribute.\n",
    "\n",
    "        # Update parameters via SGD:\n",
    "        with torch.no_grad(): # This deactivated gradient calculations\n",
    "            \n",
    "            mean_loss += loss.item() # get the raw value of a (1,) tensor\n",
    "            w -= 0.0001 * w.grad # This wouldn't be possible w/ gradient (-= is an inplace operation)\n",
    "            b -= 0.0001 * b.grad\n",
    "            w.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "            \n",
    "    # Plot the resulting line        \n",
    "    predictions = [f(x) for x in range(100)]\n",
    "    plt.plot(predictions,label=f\"epoch {epoch}\")\n",
    "\n",
    "    print('----')\n",
    "    print(\"loss:\", mean_loss/len(fake_y))\n",
    "    print('w:',w.item())\n",
    "    print('b:',b.item())\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Full pytorch tutorial: \n",
    "\n",
    "A tutorial can be found [here](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) do not hesitate to take a couple of minutes to skim read it. Plenty of [ressources](https://pytorch.org/resources) are available online. Also, you can have a look at the [extensive pytorch documentation](https://pytorch.org/docs/stable/index.html). \n",
    "\n",
    "Here, as we are defining neural networks, we mainly use the `torch.nn` module which contains most classical deep learning building blocks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used : [smallest movie-lens dataset](https://grouplens.org/datasets/movielens/)\n",
    "\n",
    "=> Just like the previous sessions\n",
    "\n",
    "\n",
    "# 1)  Load & Prepare Data\n",
    "\n",
    "To be able to embed the data easily, we need to remap  the user/items between [0->N_User] and [0->N_Items]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " #train:64535, #val:16133 ,#test:20168\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "## Load\n",
    "ratings = pd.read_csv(\"dataset/ratings.csv\")\n",
    "ratings.head(5)\n",
    "\n",
    "## Prepare Data\n",
    "user_map = {user:num for num,user in enumerate(ratings[\"userId\"].unique())}\n",
    "item_map = {item:num for num,item in enumerate(ratings[\"movieId\"].unique())}\n",
    "\n",
    "## Number of users & items\n",
    "num_users = len(user_map)\n",
    "num_items = len(item_map)\n",
    "\n",
    "ratings[\"userId\"] = ratings[\"userId\"].map(user_map)\n",
    "ratings[\"movieId\"] = ratings[\"movieId\"].map(item_map)\n",
    "\n",
    "ratings.head(5)\n",
    "\n",
    "# Creating Test/Train as before\n",
    "\n",
    "train_indexes,val_indexes,test_indexes = [],[],[]\n",
    "\n",
    "for index in range(len(ratings)):\n",
    "    if index%5 == 0:\n",
    "        test_indexes.append(index)\n",
    "    else:\n",
    "        train_indexes.append(index)\n",
    "\n",
    "        \n",
    "shuffle(train_indexes)\n",
    "num_val = int(len(train_indexes)/100*20)\n",
    "val_indexes = train_indexes[:num_val]\n",
    "train_indexes = train_indexes[num_val:]\n",
    "\n",
    "train_ratings = ratings.iloc[train_indexes].copy()\n",
    "val_ratings = ratings.iloc[val_indexes].copy()\n",
    "test_ratings = ratings.iloc[test_indexes].copy()\n",
    "\n",
    "\n",
    "print(f\" #train:{len(train_ratings)}, #val:{len(val_ratings)} ,#test:{len(test_ratings)}\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1.],requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Reproduce the baseline model with pytorch's vanilla autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal now is to reproduce the following baseline model from surprise\n",
    "\n",
    "## $$\\hat{r}_{ui} = b_{ui} = \\mu + b_u + b_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) : First, let's define the parameters\n",
    "\n",
    "You have many parameters, they are all 1-dimensional:\n",
    "- **mu:** the global mean (1,)\n",
    "- **bu:** the user means (n_users,)\n",
    "- **bi:** the item means (n_items,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.tensor([ratings[\"rating\"].mean()],requires_grad=True)\n",
    "bu = [torch.tensor([x],requires_grad=True) for x in ratings.groupby(\"userId\")[\"rating\"].mean()]\n",
    "bi = [torch.tensor([x],requires_grad=True) for x in ratings.groupby(\"movieId\")[\"rating\"].mean()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, we define two functions: \n",
    "\n",
    "- `predict(u,i)` : Will return the prediction given the (user,item) pair\n",
    "- `error(pred,real)` : Will return the MSE error of prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (TODO) Predict Function\n",
    "This function should implement this: $\\hat{r}_{ui} = b_{ui} = \\mu + b_u + b_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(u,i):\n",
    "    \n",
    "    if u < num_users: # if user exist:\n",
    "        user_mean = bu[u]\n",
    "    else:\n",
    "        user_mean = 0\n",
    "        \n",
    "    if i < num_items: # if item exist:\n",
    "        item_mean = bi[i]\n",
    "    else:\n",
    "        item_mean = 0\n",
    "        \n",
    "    return mu + user_mean + item_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) error function\n",
    "We want to use the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(pred,real):\n",
    "    return (real - pred)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The evaluation loop, without any optimization for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final train error :  49.68055539762539\n",
      "final val error :  49.87006066753322\n",
      "final test error :  49.72078751789488\n"
     ]
    }
   ],
   "source": [
    "train_e = 0\n",
    "for index, uid, mid, r, ts in train_ratings.itertuples():\n",
    "    result = predict(uid,mid)\n",
    "    train_e += error(result,r).item()\n",
    "    \n",
    "val_e = 0\n",
    "for index, uid, mid, r, ts in val_ratings.itertuples():\n",
    "    result = predict(uid,mid)\n",
    "    val_e += error(result,r).item()\n",
    "\n",
    "test_e = 0\n",
    "for index, uid, mid, r, ts in test_ratings.itertuples():\n",
    "    result = predict(uid,mid)\n",
    "    test_e += error(result,r).item()\n",
    "\n",
    "print(\"final train error : \", train_e/len(train_ratings))\n",
    "print(\"final val error : \", val_e/len(val_ratings))\n",
    "print(\"final test error : \", test_e/len(test_ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's optimize the parameters (with SGD)  by slightly modifying the previous loop\n",
    "\n",
    "### (TODO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train error :  49.68055539762539\n",
      "epoch 0 val error :  0.6631974689953236\n",
      "epoch 0 test error :  0.6442600646280782\n",
      "-----\n",
      "epoch 1 train error :  49.68055539762539\n",
      "epoch 1 val error :  0.6708243901398061\n",
      "epoch 1 test error :  0.6512817557942298\n",
      "-----\n",
      "epoch 2 train error :  49.68055539762539\n",
      "epoch 2 val error :  0.7484526932154784\n",
      "epoch 2 test error :  0.7336003113676164\n",
      "-----\n",
      "epoch 3 train error :  49.68055539762539\n",
      "epoch 3 val error :  0.7047126132921926\n",
      "epoch 3 test error :  0.6899211713570366\n",
      "-----\n",
      "epoch 4 train error :  49.68055539762539\n",
      "epoch 4 val error :  0.6932064870906215\n",
      "epoch 4 test error :  0.6709527613817793\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_error = 0\n",
    "    \n",
    "    for num,(index, uid, mid, r, ts) in enumerate(train_ratings.sample(frac=1).itertuples()):\n",
    "        result = predict(uid,mid)\n",
    "        ex_error = error(result,r)\n",
    "        train_error += ex_error.item()\n",
    "        ex_error.backward()\n",
    "\n",
    "        if num % batch_size == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                mu -= lr * mu.grad\n",
    "                bu[uid] -= lr * bu[uid].grad\n",
    "                bi[mid] -= lr * bi[mid].grad\n",
    "\n",
    "                # Manually zero the gradients after updating weights\n",
    "                mu.grad.zero_()\n",
    "                bu[uid].grad.zero_()\n",
    "                bi[mid].grad.zero_()\n",
    "\n",
    "\n",
    "    print(f\"epoch {epoch} train error : \", train_e/len(train_ratings))\n",
    "    \n",
    "    val_e = 0\n",
    "    for index, uid, mid, r, ts in val_ratings.itertuples():\n",
    "        result = predict(uid,mid)\n",
    "        val_e += error(result,r).item()\n",
    "\n",
    "    print(f\"epoch {epoch} val error : \", val_e/len(val_ratings))\n",
    "\n",
    "\n",
    "    test_e = 0\n",
    "    for index, uid, mid, r, ts in test_ratings.itertuples():\n",
    "        result = predict(uid,mid)\n",
    "        test_e += error(result,r).item()\n",
    "\n",
    "    print(f\"epoch {epoch} test error : \", test_e/len(test_ratings))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Pytorch (.nn) Modules\n",
    "\n",
    "Instead of having to define everything by hand, pytorch has several usefull abstractions:\n",
    "\n",
    "- `nn.Module()` -> To define the model and the forward computation\n",
    "- `torch.utils.data.DataLoader` -> To create the data pipeline\n",
    "\n",
    "To explore these modules, we'll do the following model:\n",
    "\n",
    "##  Classic SVD (with mean)\n",
    "\n",
    "To see how it works, we propose to implement a simple SVD:\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu) }_\\text{regularization} $$\n",
    "\n",
    "where prediction is done in the following way:\n",
    "### $$r_{ui} = \\mu + U_u.I_i $$\n",
    "\n",
    "where $\\mu$ is the global mean,  $U_u$ a user embedding and $I_i$ an item embedding\n",
    "\n",
    "### STEPS:\n",
    " To implement such model in pytorch, we need to do multiple things:\n",
    " \n",
    " - (1) model definition\n",
    " - (2) loss function\n",
    " - (3) evaluation\n",
    " - (4) training/eval loop\n",
    "\n",
    "\n",
    "#### (1) Model definition\n",
    "\n",
    "A model class typically extends `nn.Module`, the Neural network module. It is a convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
    "\n",
    "One should define two functions: `__init__` and `forward`.\n",
    "\n",
    "- `__init__` is used to initialize the model parameters\n",
    "- `forward` is the net transformation from input to output. In fact, when doing `moduleClass(input)` you call this method.\n",
    "\n",
    "##### (a) Initialization\n",
    "\n",
    "Our model has different weigths:\n",
    "\n",
    "- the user profiles (also called user embeddings) $U$\n",
    "- the item profiles (also called user embeddings) $I$\n",
    "- the mean bias $\\mu$\n",
    "\n",
    "\n",
    "##### (b) input to output operation\n",
    "Technically, the prediction as defined earlier can be seen as just a dot product between two embeddings $U_u$ and $I_i$ plus the mean rating:\n",
    "\n",
    "- `torch.sum(embed_u*embed_i,1) + self.mean` is equivalent to $r_{ui} = \\mu + U_u.I_i $ \n",
    "- the `.squeeze(1)` operation is a shape operation to remove the dimension 1 (indexing starts at 0) akin to reshaping the matrix from `(batch_size,1,latent_size)` to `(batch_size,latent_size)`\n",
    "- for reference, the inverse operation is `.unsqueeze()`\n",
    "- we return weights to regularize them\n",
    "\n",
    "\n",
    "### (TODO) Just to make sure you were following: complete the following `forward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let's create the datasets following  (Object w/ __getitem__(index) and __len()__, i.e lists ;)\n",
    "prep_train = [(tp.userId,tp.movieId,tp.rating) for tp in train_ratings.itertuples()]\n",
    "prep_val = [(tp.userId,tp.movieId,tp.rating) for tp in val_ratings.itertuples()]\n",
    "prep_test = [(tp.userId,tp.movieId,tp.rating) for tp in test_ratings.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# The model define as a class, inheriting from nn.Module\n",
    "class ClassicMF(nn.Module):\n",
    "    \n",
    "    #(a) Init\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(ClassicMF, self).__init__()\n",
    "        \n",
    "        #Embedding layers\n",
    "        self.users = nn.Embedding(nb_users, latent_size)        \n",
    "        self.items = nn.Embedding(nb_items, latent_size)\n",
    "        #The mean bias\n",
    "        self.mean = nn.Parameter(torch.FloatTensor(1,).fill_(3))\n",
    "        \n",
    "        #initialize weights with very small values\n",
    "        nn.init.normal_(self.users.weight,0,0.01)\n",
    "        nn.init.normal_(self.items.weight,0,0.01)\n",
    "\n",
    "    \n",
    "    # (b) How we compute the prediction (from input to output)\n",
    "    def forward(self, user, item): ## method called when doing ClassicMF(user,item)\n",
    "        \n",
    "        embed_u,embed_i = self.users(user).squeeze(1),self.items(item).squeeze(1)\n",
    "        out =  torch.sum(embed_u*embed_i,1) + self.mean\n",
    "\n",
    "        return out, embed_u, embed_i, self.mean  # We return prediction + weights to regularize them\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2-4) full train loop\n",
    "\n",
    "The train loop is organized around the [Dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class which Combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset.\n",
    "\n",
    "We just redefine a collate function\n",
    "\n",
    "> collate_fn (callable, optional) – merges a list of samples to form a mini-batch.\n",
    "\n",
    "\n",
    "**NOTE:** The dataset argument can be a list instead of a \"Dataset\" instance (works by duck typing)\n",
    "    \n",
    "\n",
    "##### The train loop sequence is the following:\n",
    "    \n",
    "[Dataset ==Dataloader==> Batch (not prepared) ==collate_fn==> Batch (prepared) ==Model.forward==> Prediction =loss_fn=> loss <-> truth \n",
    "\n",
    "1] PREDICT\n",
    "- (a) The dataloader samples training exemples from the dataset (which is a list)\n",
    "- (b) The collate_fn prepares the minibatch of training exemples\n",
    "- (c) The prediction is made by feeding the minibatch in the model\n",
    "- (d) The loss is computed on the prediction via a loss function\n",
    "\n",
    "2] OPTIMIZE\n",
    "- (e) Gradients are computed by automatic backard propagation\n",
    "- (f) Parameters are updated using computed gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "epoch 0 mse (train/val/test) 1.048 / 0.913 / 0.898\n",
      "-------------------------\n",
      "epoch 1 mse (train/val/test) 0.786 / 0.816 / 0.796\n",
      "-------------------------\n",
      "epoch 2 mse (train/val/test) 0.605 / 0.791 / 0.773\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# HyperParameters\n",
    "n_epochs = 3\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "lr = 0.01\n",
    "reg = 0.001\n",
    "\n",
    "\n",
    "#(b) Collate function => Creates tensor batches to feed model during training\n",
    "# It can be removed if data is already tensors (torch or numpy ;)\n",
    "def tuple_batch(l):\n",
    "    '''\n",
    "    input l: list of (user,item,rating tuples)\n",
    "    output: formatted batches (in torch tensors)\n",
    "\n",
    "    takes n-tuples and create batch\n",
    "    text -> seq word #id\n",
    "    '''\n",
    "    users, items, ratings = zip(*l) \n",
    "    users_t = torch.LongTensor(users)\n",
    "    items_t = torch.LongTensor(items)\n",
    "    ratings_t = torch.FloatTensor(ratings)\n",
    "    \n",
    "    return users_t, items_t, ratings_t\n",
    "    \n",
    "\n",
    "\n",
    "#(d) Loss function => Combines MSE and L2\n",
    "def loss_func(pred,ratings_t,reg,*params):\n",
    "    '''\n",
    "    mse loss combined with l2 regularization.\n",
    "    params assumed 2-dimension\n",
    "    '''\n",
    "    mse = F.mse_loss(pred,ratings_t,reduction='sum')\n",
    "    l2 = 0\n",
    "    for p in params:\n",
    "        l2 += torch.mean(p.norm(2,-1))\n",
    "        \n",
    "    return (mse/pred.size(0)) + reg*l2 , mse\n",
    "    \n",
    "#\n",
    "# Training script starts here\n",
    "#    \n",
    "\n",
    "\n",
    "model = ClassicMF(num_users,num_items,num_feat)\n",
    "\n",
    "\n",
    "\n",
    "# (a) dataloader will sample data from datasets using collate_fn tuple_batch\n",
    "dataloader_train = DataLoader(prep_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_val = DataLoader(prep_val, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_test = DataLoader(prep_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train loop\n",
    "for e in range(n_epochs):\n",
    "    mean_loss = [0,0,0] #train/val/test\n",
    "\n",
    "    ## Training loss (the one we train with)\n",
    "    \n",
    "    for users_t,items_t,ratings_t in dataloader_train:\n",
    "        model.train() # set the model on train mode\n",
    "        model.zero_grad() # reset gradients\n",
    "\n",
    "        #(c) predictions are made by the model\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        \n",
    "        #(d) loss computed on predictions, we added regularization\n",
    "        loss,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "        \n",
    "        loss.backward() #(e) backpropagating to get gradients\n",
    "        \n",
    "        mean_loss[0] += mse_loss\n",
    "        optimizer.step() #(f) updating parameters\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ## Validation loss (no training)\n",
    "        for users_t,items_t,ratings_t in dataloader_val:\n",
    "\n",
    "            model.eval() # Inference mode\n",
    "            pred,*params = model(users_t,items_t)\n",
    "            _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "\n",
    "            mean_loss[1] += mse_loss    \n",
    "\n",
    "        ## Test loss (no training)\n",
    "\n",
    "        for users_t,items_t,ratings_t in dataloader_test:\n",
    "            model.eval()\n",
    "            pred,*params = model(users_t,items_t)\n",
    "            _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "\n",
    "            mean_loss[2] += mse_loss    \n",
    "\n",
    "    print(\"-\"*25)\n",
    "    print(\"epoch\",e, \"mse (train/val/test)\", round((mean_loss[0]/len(prep_train)).item(),3),\"/\",  round((mean_loss[1]/len(prep_val)).item(),3),\"/\",  round((mean_loss[2]/len(prep_test)).item(),3))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Your turn) Koren 2009 model:\n",
    "\n",
    "Here, this model simply adds a bias for each user and for each item\n",
    "\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu+ \\mu_i+\\mu_u))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu  + \\mu+ \\mu_i+\\mu_u) }_\\text{regularization} $$\n",
    "\n",
    "\n",
    "### $$r_{ui} = \\mu + \\mu_i + \\mu_u + U_u.I_i $$\n",
    "\n",
    "### TODO:\n",
    "\n",
    "- (a) complete the model initialization\n",
    "- (b) complete the forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KorenMF(nn.Module):\n",
    "\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(KorenMF, self).__init__()\n",
    "        \n",
    "        self.users = nn.Embedding(nb_users, latent_size)\n",
    "        self.items = nn.Embedding(nb_items, latent_size)\n",
    "        self.umean = nn.Embedding(nb_users,1)\n",
    "        self.imean = nn.Embedding(nb_items,1)\n",
    "        self.gmean = nn.Parameter(torch.FloatTensor(1,).fill_(3))\n",
    "\n",
    "        nn.init.normal_(self.users.weight,0,0.01)\n",
    "        nn.init.normal_(self.items.weight,0,0.01)\n",
    "        nn.init.normal_(self.umean.weight,2,1)\n",
    "        nn.init.normal_(self.imean.weight,2,1)\n",
    "        \n",
    "        \n",
    "    def forward(self, user,item):\n",
    "        embed_u,embed_i = self.users(user).squeeze(1) , self.items(item).squeeze(1)\n",
    "        umean, imean = self.umean(user) , self.imean(item)\n",
    "        out = torch.sum(embed_u*embed_i,1) + self.gmean + umean.view(-1) + imean.view(-1)\n",
    "\n",
    "        return out , embed_u, embed_i, umean , imean , self.gmean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) Here, train loop stays the same, you only have to change the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "epoch 0 mse (train/val/test) 5.307 / 1.847 / 1.863\n",
      "-------------------------\n",
      "epoch 1 mse (train/val/test) 1.402 / 1.245 / 1.263\n",
      "-------------------------\n",
      "epoch 2 mse (train/val/test) 0.965 / 1.016 / 1.027\n",
      "-------------------------\n",
      "epoch 3 mse (train/val/test) 0.766 / 0.938 / 0.948\n",
      "-------------------------\n",
      "epoch 4 mse (train/val/test) 0.666 / 0.908 / 0.916\n",
      "-------------------------\n",
      "epoch 5 mse (train/val/test) 0.589 / 0.9 / 0.905\n",
      "-------------------------\n",
      "epoch 6 mse (train/val/test) 0.517 / 0.898 / 0.902\n",
      "-------------------------\n",
      "epoch 7 mse (train/val/test) 0.444 / 0.905 / 0.908\n",
      "-------------------------\n",
      "epoch 8 mse (train/val/test) 0.373 / 0.918 / 0.927\n",
      "-------------------------\n",
      "epoch 9 mse (train/val/test) 0.312 / 0.941 / 0.949\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "lr = 0.01\n",
    "reg = 0.001\n",
    "\n",
    "\n",
    "\n",
    "def tuple_batch(l):\n",
    "    '''\n",
    "    input l: list of (user,item,review, rating tuples)\n",
    "    output: formatted batches (in torch tensors)\n",
    "\n",
    "    takes n-tuples and create batch\n",
    "    text -> seq word #id\n",
    "    '''\n",
    "    users, items,ratings = zip(*l)\n",
    "    users_t = torch.LongTensor(users)\n",
    "    items_t = torch.LongTensor(items)\n",
    "    ratings_t = torch.FloatTensor(ratings)\n",
    "    \n",
    "    return users_t,items_t,ratings_t\n",
    "\n",
    "\n",
    "def loss_func(pred,ratings_t,reg,*params):\n",
    "    '''\n",
    "    mse loss combined with l2 regularization.\n",
    "    params assumed 2-dimension\n",
    "    '''\n",
    "    mse = F.mse_loss(pred,ratings_t,reduction=\"sum\")\n",
    "    l2 = 0\n",
    "    for p in params:\n",
    "        l2 += torch.mean(p.norm(2,-1))\n",
    "        \n",
    "    return (mse/pred.size(0)) + reg*l2 , mse\n",
    "    \n",
    "\n",
    "model =  KorenMF(num_users,num_items,num_feat)\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(prep_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_val = DataLoader(prep_val, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_test = DataLoader(prep_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    mean_loss = [0,0,0] #train/val/test\n",
    "\n",
    "    for users_t,items_t,ratings_t in dataloader_train:\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        loss,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "        loss.backward()\n",
    "        \n",
    "        mean_loss[0] += mse_loss\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "\n",
    "    for users_t,items_t,ratings_t in dataloader_val:\n",
    "        model.eval()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "    \n",
    "        mean_loss[1] += mse_loss    \n",
    "        \n",
    "    for users_t,items_t,ratings_t in dataloader_test:\n",
    "        model.eval()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "    \n",
    "        mean_loss[2] += mse_loss    \n",
    "\n",
    "    print(\"-\"*25)\n",
    "    print(\"epoch\",e, \"mse (train/val/test)\", round((mean_loss[0]/len(prep_train)).item(),3),\"/\",  round((mean_loss[1]/len(prep_val)).item(),3),\"/\",  round((mean_loss[2]/len(prep_test)).item(),3))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch's keras: Pytorch-Lightning\n",
    "\n",
    "Pytorch lightning is a easy to use framework for Pytorch. To start a new project you just need to define two files:\n",
    "\n",
    "- a LightningModule (which inherits `pl.LightningModule`)\n",
    "- a Trainer file. \n",
    "\n",
    "By defining those two files, you get:\n",
    "- Checkpointing\n",
    "- Debugging\n",
    "- Distributed training\n",
    "- Experiment Logging\n",
    "- Training loop\n",
    "- Validation loop\n",
    "- Testing loop\n",
    "\n",
    "## Let's try with the same but different Koren 2009 model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### $$r_{ui} = \\mu + \\mu_i + \\mu_u + U_u.I_i $$\n",
    "\n",
    "Where the goal is to minimize the following loss\n",
    "\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu+ \\mu_i+\\mu_u))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu  + \\mu+ \\mu_i+\\mu_u) }_\\text{regularization} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Complete the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class LightningKorenMF(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(LightningKorenMF, self).__init__()\n",
    "        \n",
    "        self.reg = 0.001\n",
    "        \n",
    "        self.users = nn.Embedding(nb_users, latent_size)\n",
    "        self.items = nn.Embedding(nb_items, latent_size)\n",
    "        self.umean = nn.Embedding(nb_users,1) # not sure\n",
    "        self.imean = nn.Embedding(nb_items,1) # not sure\n",
    "        self.gmean =  nn.Parameter(torch.FloatTensor(1,).fill_(3))\n",
    "\n",
    "        nn.init.normal_(self.users.weight,0,0.01)\n",
    "        nn.init.normal_(self.items.weight,0,0.01)\n",
    "        nn.init.normal_(self.umean.weight,0.1,0.1)\n",
    "        nn.init.normal_(self.imean.weight,0.1,0.1)\n",
    "        \n",
    "\n",
    "    def forward(self, user,item):\n",
    "        embed_u,embed_i = self.users(user).squeeze(1) , self.items(item).squeeze(1)\n",
    "        umean, imean = self.umean(user) , self.imean(item)\n",
    "        out = torch.sum(embed_u*embed_i,1) + self.gmean + umean.view(-1) + imean.view(-1)\n",
    "        \n",
    "        return out , embed_u, embed_i, umean , imean , self.gmean\n",
    "\n",
    "    \n",
    "    def my_loss_func(self, pred,ratings_t,reg,*params):\n",
    "        '''\n",
    "        mse loss combined with l2 regularization.\n",
    "        params assumed 2-dimension\n",
    "        '''        \n",
    "        mse = F.mse_loss(pred,ratings_t)\n",
    "        l2 = 0\n",
    "        for p in params:\n",
    "            l2 += torch.mean(p.norm(2,-1))\n",
    "\n",
    "        return mse + reg*l2 , mse\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # REQUIRE\n",
    "        users_t,items_t,ratings_t = batch\n",
    "        pred , *params = self.forward(users_t,items_t) \n",
    "        loss,mse = self.my_loss_func(pred,ratings_t,self.reg,*params)\n",
    "\n",
    "        return {'loss':loss,\"mse\":mse}\n",
    "    \n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        return {\"val_mse\":self.training_step(batch,batch_nb)[\"mse\"]}\n",
    "    \n",
    "    def validation_end(self,outputs):\n",
    "        return {\"progress_bar\":{\"val_mse\":torch.tensor([output['val_mse'] for output in outputs]).mean().item()}}\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        return {\"test_mse\":self.training_step(batch,batch_nb)[\"mse\"]}\n",
    "    \n",
    "    def test_end(self,outputs):\n",
    "        res = {\"progress_bar\":{\"test_mse\":torch.tensor([output['test_mse'] for output in outputs]).mean().item()}}\n",
    "        print(res)\n",
    "        return res\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.002)\n",
    "    \n",
    "    def tuple_batch(self,l):\n",
    "        '''\n",
    "        input l: list of (user,item,rating tuples)\n",
    "        output: formatted batches (in torch tensors)\n",
    "\n",
    "        takes n-tuples and create batch\n",
    "        text -> seq word #id\n",
    "        '''\n",
    "        users, items, ratings = zip(*l) \n",
    "        users_t = torch.LongTensor(users)\n",
    "        items_t = torch.LongTensor(items)\n",
    "        ratings_t = torch.FloatTensor(ratings)\n",
    "\n",
    "        return users_t, items_t, ratings_t\n",
    "    \n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # REQUIRED\n",
    "        return DataLoader(prep_train,collate_fn=self.tuple_batch ,num_workers=0, batch_size=32)\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        return DataLoader(prep_val,collate_fn=self.tuple_batch,num_workers=0, batch_size=32)\n",
    "\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        return DataLoader(prep_test,collate_fn=self.tuple_batch,num_workers=0, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | users | Embedding | 30.5 K\n",
      "1 | items | Embedding | 486 K \n",
      "2 | umean | Embedding | 610   \n",
      "3 | imean | Embedding | 9.7 K \n",
      "------------------------------------\n",
      "527 K     Trainable params\n",
      "0         Non-trainable params\n",
      "527 K     Total params\n",
      "2.108     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b2ba40d3634dc9ac094795a2c2fc9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35217ebe116544baa51fcd38330d172f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_mse': 0.3173159062862396}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_mse': 0.3173159062862396}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "model = LightningKorenMF(num_users,num_items,50)\n",
    "\n",
    "# most basic trainer, uses good defaults\n",
    "trainer = Trainer(max_epochs=10)    \n",
    "trainer.fit(model)\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still got time ?\n",
    "\n",
    "[Take a glance at the documentation](https://williamfalcon.github.io/pytorch-lightning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
